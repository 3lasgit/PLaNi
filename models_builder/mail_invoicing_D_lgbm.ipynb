{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mail Model per hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing mail datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import os\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scikit-learn.model_selection import cross_val_score\n",
    "from scikit-learn.metrics import make_scorer\n",
    "from scikit-learn.metrics import mean_absolute_error as mae\n",
    "from scikit-learn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "from gives import *\n",
    "\n",
    "def get_db_from_prefix(db_prefix: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the name of the database with the shared suffix if it exists otherwise\n",
    "    return the input db_prefix.\n",
    "    You can get the db_prefix in the overview tab of the dataset in DataStudio\n",
    "    from the \"Glue database\" field in the \"AWS information\" section.\n",
    "    \"\"\"\n",
    "    shared_suffixed_name = f'{db_prefix}_shared_'\n",
    "    database_names = wr.catalog.databases()[\"Database\"]\n",
    "    return next(filter(lambda x: x.startswith(shared_suffixed_name), database_names), db_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = wr.catalog.databases()\n",
    "db_prefix = 'im_commercial_excellence_sales_eu_shared_oeflowob' \n",
    "database = get_db_from_prefix(db_prefix)\n",
    "tables = wr.catalog.tables(database=database, limit=50)\n",
    "table = tables[\"Table\"].iloc[5]\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mail = wr.athena.read_sql_query(\n",
    "    sql=f'''SELECT case_number, consideration_start, reason\n",
    "            FROM \"{table}\"\n",
    "            WHERE to_email_address = 'contact.alfi@airliquide.com'\n",
    "              AND consideration_start >= TIMESTAMP '2020-05-01 00:00:00' ''',\n",
    "    database=database,\n",
    "    ctas_approach=False,\n",
    "    workgroup=os.getenv(\"ATHENA_WORKGROUP\")\n",
    ")#.sort_valuers('consideration_start',ascending=False)\n",
    "\n",
    "df_mail.fillna('vide',inplace=True)\n",
    "df_mail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The **f** before the string in the command **f'SELECT case_number, consideration_start, reason FROM \"{table}\" WHERE to_email_address = \\'contact.alfi@airliquide.com\\' AND consideration_start >= TIMESTAMP \\'2020-06-01 00:00:00\\''** is used to create a formatted string literal, also known as an f-string in Python.\n",
    "\n",
    "The **f** prefix allows you to include expressions inside curly braces **{}** within the string, which will be evaluated and replaced with their values. In this case, the **{table}** is a placeholder for the value of the variable table, which will be substituted with the actual table name when the string is formatted.\n",
    "\n",
    "The backslash **\\\\** is used as an escape character in this context. It is used to escape the single quotes (**'**) within the string literal. By adding a backslash before the single quote (**\\'**), it indicates that the single quote is part of the string and should not be interpreted as the end of the string.\n",
    "\n",
    "For example, in the string **\\'contact.alfi@airliquide.com\\'**, the backslashes before the single quotes ensure that the single quotes are treated as literal characters within the string, rather than as string delimiters.\n",
    "\n",
    "This way, the **f** prefix and the backslash escape **\\\\** characters help in constructing the SQL query as a valid string with the necessary substitutions and escaping of special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring data: cleaning and analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mail.reason.unique()\n",
    "#df_mail.loc[df_mail['reason']=='Payment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reason_table = df_mail.pivot_table(values='case_number', columns='reason', aggfunc='count')\n",
    "px.histogram(df_mail,x='reason').update_xaxes(categoryorder='total descending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mail_2023=df_mail.set_index('consideration_start').sort_index().loc['2023':,:]\n",
    "px.histogram(df_mail_2023, x='reason').update_xaxes(categoryorder='total descending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapp_link=pd.read_csv('../data/mapping_\\'reason\\'_mail.csv')\n",
    "print(mapp_link.to_dict())\n",
    "dict_link={'logistic': ['Logistics and material orders', 'Distribution/logistic', 'Liquid ordering & Delivery', \n",
    "                        'C4 - Liquid ordering & Delivery', 'C3 - Cylinder Ordering & Delivery', 'Cylinder ordering & Delivery', \n",
    "                        'C5 - Equipement & Installations'],\n",
    " 'ecopass': 'Ecopass',\n",
    " 'distributor': 'Support to distributors',\n",
    " 'contract': ['Contract & Customer Relationship', 'Customer Digital Platform Support', 'Copy of document', \n",
    "              'Administration & Master Data', 'Maintenance', 'Product Quality', 'Cylinders packaging and maintenance', 'Quality', \n",
    "              'Maintenance & Services', 'C2 - Contract & Cust. Relationship', 'Installations Hard goods', 'Transactional Survey', \n",
    "              'Administrative'],\n",
    " 'invoicing': ['Invoicing & Payment', 'Payment', 'C7 - Invoicing & Payment', 'C8 - Payment', 'Cash Collection'],\n",
    " 'others': ['vide', 'Others', 'Other informations', 'Mistake / Silence / Joke', 'Covid-19', 'Other Group Company', 'Suggestions', \n",
    "            'Relationship Survey', 'S9 - IT/IS Systems', 'S8 - Cylinders Package & Maintenance', 'C6 - Maintenance & Services', \n",
    "            'S5 - Product Quality']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reas_cl(value) :\n",
    "    if value in ['Logistics and material orders', 'Distribution/logistic', 'Liquid ordering & Delivery', \n",
    "                 'C4 - Liquid ordering & Delivery', 'C3 - Cylinder Ordering & Delivery', 'Cylinder ordering & Delivery', \n",
    "                 'C5 - Equipement & Installations'] :\n",
    "        value = 'logistic'\n",
    "    elif value in ['Ecopass'] :\n",
    "        value = 'ecopass'\n",
    "    elif value in ['Support to distributors'] :\n",
    "        value = 'distributor'\n",
    "    elif value in ['Contract & Customer Relationship', 'Customer Digital Platform Support', 'Copy of document', \n",
    "                   'Administration & Master Data', 'Maintenance', 'Product Quality', 'Cylinders packaging and maintenance', 'Quality', \n",
    "                   'Maintenance & Services', 'C2 - Contract & Cust. Relationship', 'Installations Hard goods', 'Transactional Survey', \n",
    "                   'Administrative','Commercial'] :\n",
    "        value = 'contract'\n",
    "    elif value in ['Invoicing & Payment', 'Payment', 'C7 - Invoicing & Payment', 'C8 - Payment', 'Cash Collection'] :\n",
    "        value = 'invoicing'\n",
    "    elif value in ['vide', 'Others', 'Other informations', 'Mistake / Silence / Joke', 'Covid-19', 'Other Group Company', 'Suggestions', \n",
    "                   'Relationship Survey', 'S9 - IT/IS Systems', 'S8 - Cylinders Package & Maintenance', 'C6 - Maintenance & Services', \n",
    "                   'S5 - Product Quality', 'S1 - Risks & Environment'] :\n",
    "        value = 'others'\n",
    "    return value\n",
    "\n",
    "def in_list(value, list_) :\n",
    "    if value in list_ :\n",
    "        value = True\n",
    "    else :\n",
    "        value = False\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mail_clean = df_mail.copy()\n",
    "df_mail_clean.reason = df_mail.reason.apply(reas_cl)\n",
    "df_mail_clean.reason.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analysing statistics and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig=px.histogram(df_mail_clean, x='reason').update_xaxes(categoryorder='total descending')\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_color=\"orange\",\n",
    "    title_font_size=20,\n",
    "    #legend_title_font_color=\"green\",\n",
    "    \n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Répartition des mails par équipe\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_clean_2023=df_mail_clean.set_index('consideration_start').sort_index().loc['2023':,:]\n",
    "fig=px.histogram(df_clean_2023, x='reason',labels={\"reason\": \"Equipe\", \"count\" : \"Volumes\"}).update_xaxes(categoryorder='total descending')\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_color=\"orange\",\n",
    "    title_font_size=20,\n",
    "    #legend_title_font_color=\"green\",\n",
    "    \n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Répartition des mails par équipe en 2023\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Others' will be considered as an epsilon error. Two ways of modelating it : one with an lgbm for every categories including one for others or one regression model where others would be considered as an epsilon error as it was said earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Zoom sur 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_others=df_mail.loc[df_mail.reason.apply(in_list, list_=['vide', 'Others', 'Other informations', 'Mistake / Silence / Joke', 'Covid-19', 'Other Group Company', 'Suggestions', \n",
    "                   'Relationship Survey', 'S9 - IT/IS Systems', 'S8 - Cylinders Package & Maintenance', 'C6 - Maintenance & Services', \n",
    "                   'S5 - Product Quality', 'S1 - Risks & Environment']),:]#.set_index('consideration_start').sort_index().loc['2023':,:]\n",
    "fig=px.histogram(df_others, x='reason',labels={\"reason\": \"Equipe\", \"count\" : \"Volumes\"}).update_xaxes(categoryorder='total descending')\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_color=\"orange\",\n",
    "    title_font_size=20,\n",
    "    #legend_title_font_color=\"green\",\n",
    "    \n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Zoom sur 'others'\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_others=df_mail.loc[df_mail.reason.apply(in_list, list_=['vide', 'Others', 'Other informations', 'Mistake / Silence / Joke', 'Covid-19', 'Other Group Company', 'Suggestions', \n",
    "                   'Relationship Survey', 'S9 - IT/IS Systems', 'S8 - Cylinders Package & Maintenance', 'C6 - Maintenance & Services', \n",
    "                   'S5 - Product Quality', 'S1 - Risks & Environment']),:].set_index('consideration_start').sort_index().loc['2023':,:]\n",
    "fig=px.histogram(df_others, x='reason',labels={\"reason\": \"Equipe\", \"count\" : \"Volumes\"}).update_xaxes(categoryorder='total descending')\n",
    "fig.update_layout(\n",
    "    font_family=\"Arial\",\n",
    "    font_color=\"black\",\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_color=\"orange\",\n",
    "    title_font_size=20,\n",
    "    #legend_title_font_color=\"green\",\n",
    "    \n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Zoom sur 'others' en 2023\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Forming dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dico={}\n",
    "for name in df_mail_clean.reason.unique() :\n",
    "    dico[\"df_\" + str(name)] = df_mail_clean.loc[df_mail_clean.reason==name]\n",
    "dico['df_invoicing'].head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#par heure \n",
    "# d_new={}\n",
    "# for i in dico : \n",
    "#     d_new[i] = df_agg(dico[i],freq='H',y_name='volume', key='consideration_start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#par jour\n",
    "d_new={}\n",
    "for i in dico : \n",
    "    key='consideration_start'\n",
    "    d_new[i] = dico[i].groupby(pd.Grouper(key='consideration_start', freq='D')).size().reset_index(name='volume')\n",
    "    # Ajout des features temporelles détaillées\n",
    "    d_new[i]=d_new[i].assign(year=d_new[i][key].dt.isocalendar()['year'], \n",
    "                 week=d_new[i][key].dt.isocalendar()['week'],\n",
    "                 day=d_new[i][key].dt.isocalendar()['day'],\n",
    "                 #quarter=d_new[i]['Consideration Start'].dt.quarter,\n",
    "                 month=d_new[i][key].dt.month,\n",
    "                 day_mon=d_new[i][key].dt.day,)\n",
    "                 #hour=d_new[i][key].dt.hour*100+d_new[i][key].dt.minute)\n",
    "\n",
    "    # Conversion de la colonne de date en index\n",
    "    d_new[i].set_index(key, inplace=True)\n",
    "\n",
    "    # Éjection des samedis et dimanches lorsque l'on est en phone only\n",
    "    d_new[i].drop(index=d_new[i].loc[(d_new[i]['day']==6) | (d_new[i]['day']==7)].index, inplace=True)\n",
    "\n",
    "    # Conversion dans le type adéquat pour LGBM\n",
    "    d_new[i] = d_new[i].astype(int)\n",
    "\n",
    "    # Rajoute les vacances, jours fériés et d-1 fériés\n",
    "    d_new[i]=d_new[i].assign(vac_A=d_new[i].apply(lambda x : d.is_holiday_for_zone(datetime.date(x['year'],x['month'],x['day_mon']),'A'),axis=1),\n",
    "                 vac_B=d_new[i].apply(lambda x : d.is_holiday_for_zone(datetime.date(x['year'],x['month'],x['day_mon']),'B'),axis=1),\n",
    "                 vac_C=d_new[i].apply(lambda x : d.is_holiday_for_zone(datetime.date(x['year'],x['month'],x['day_mon']),'C'),axis=1),\n",
    "                 ouvre=d_new[i].apply(lambda x : cal.is_working_day(datetime.date(x['year'],x['month'],x['day_mon'])),axis=1),\n",
    "                 # begin=d_new[i].apply(lambda x : 800<=x['hour']<900, axis=1),\n",
    "                 # lunch=d_new[i].apply(lambda x : 1200<=x['hour']<1300, axis=1),\n",
    "                 # end=d_new[i].apply(lambda x : 1700<=x['hour']<1800, axis=1)\n",
    "                 )\n",
    "\n",
    "\n",
    "    d_new[i]['d-1_fer']=((d_new[i]['ouvre'].shift(1,fill_value=True)-1)*-1).apply(bool)\n",
    "    d_new[i].loc[(d_new[i]['day']==1) & (d_new[i]['d-1_fer'])==True,'d-1_fer']=False \n",
    "\n",
    "    # Éjection des samedis et dimanches lorsque l'on est en phone only ainsi que les jours fériés\n",
    "    d_new[i].drop(index=d_new[i].loc[d_new[i].ouvre==False].index, inplace=True)\n",
    "    d_new[i].drop(columns='ouvre', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=d_new['df_invoicing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(name=\"Vraies valeurs\", x=df.index, y=df.volume, mode='lines+markers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models' Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(df, train_beg='2020-05-01', train_end='2022-05-31', test_beg='2022-06-01', test_end='2023-06-09', y_name='volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    #'cat_feature=name' : ['year', 'week', 'day','quarter', 'month', 'day_mon', 'vac_A', 'vac_B', 'vac_C', 'lunch', 'd-1_fer']\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test), (X_train, y_train)], verbose=0)\n",
    "lgb.plot_metric(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### LGBM_hopt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "\n",
    "list_obj = ['tweedie','mae','regression']\n",
    "\n",
    "lgb_reg_params = { ##définition perso des valeurs possibles\n",
    "    'objective' :       'regression',\n",
    "    #'metric':           hp.choice('metric', list_obj), #‘perso_pic’\n",
    "    'learning_rate':    hp.uniform('learning_rate',0.001,0.5), # ou ,0.1,1)\n",
    "    #'num_iterations':   hp.choice('num_iterations',       np.arange(5, 200, 1, dtype=int)),\n",
    "    'num_leaves':       hp.choice('num_leaves',       np.arange(2, 89, 1, dtype=int)),\n",
    "    'max_depth':        hp.choice('max_depth',        np.arange(2, 100, 1, dtype=int)),\n",
    "    #'min_child_weight': hp.choice('min_child_weight', np.arange(1, 50, 1, dtype=int)),\n",
    "    #'colsample_bytree': hp.uniform('colsample_bytree',0.4,1),\n",
    "    #'subsample':        hp.uniform('subsample', 0.6, 1),\n",
    "    #'min_split_gain':   hp.uniform('min_split_gain', 0, 1),\n",
    "}\n",
    "# lgb_reg_params_comp = {\n",
    "#     'objective' :       hp.choice('objective', ['tweedie','mae','regression']), #‘perso_pic’\n",
    "#     'learning_rate':    hp.uniform('learning_rate',0.1,1),\n",
    "#     'num_leaves':       hp.choice('num_leaves',       np.arange(2, 200, 1, dtype=int)),\n",
    "#     'max_depth':        hp.choice('max_depth',        np.arange(2, 100, 1, dtype=int)),\n",
    "#     'min_child_weight': hp.choice('min_child_weight', np.arange(1, 50, 1, dtype=int)),\n",
    "#     'colsample_bytree': hp.uniform('colsample_bytree',0.4,1),\n",
    "#     'subsample':        hp.uniform('subsample', 0.6, 1),\n",
    "#     'min_split_gain':   hp.uniform('min_split_gain', 0, 1),\n",
    "#     'n_estimators':     5\n",
    "# } \n",
    "##### mettre les espaces des paramètres compris dans Param_LGBM\n",
    "def f(params) : \n",
    "    lgbm = lgb.LGBMRegressor(n_jobs=-1,early_stopping_rounds=None,**params)\n",
    "    score = cross_val_score(lgbm, X_train, y_train, cv=2,scoring=mae_scorer,n_jobs=-1).mean() ## ['regression', 'tweedie', 'mape'] je peux les mettre ici sinon en utilant scikit-learn...cross_validate\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "result = fmin(\n",
    "    fn=f,                           # objective function\n",
    "    space=lgb_reg_params,   # parameter space\n",
    "    algo=tpe.suggest,               # surrogate algorithm\n",
    "    max_evals=500,                  # no. of evaluations\n",
    "    trials=trials,                   # trials object that keeps track of the sample results (optional)\n",
    "    verbose=1\n",
    ")\n",
    "result\n",
    "result = space_eval(lgb_reg_params, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = result.copy()\n",
    "#params['objective']='tweedie'\n",
    "params['num_iterations']=200\n",
    "\n",
    "lgb_hopt = lgb.LGBMRegressor(**params)\n",
    "lgb_hopt.fit(X_train, y_train, eval_set=[(X_test, y_test), (X_train, y_train)], verbose=0)\n",
    "lgb.plot_metric(lgb_hopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### LBGM_hopt_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.1630936299000586,\n",
    " 'max_depth': 60,\n",
    " 'num_leaves': 2,\n",
    " 'objective': 'regression'}\n",
    "\n",
    "# {'learning_rate': 0.010059706014775412,\n",
    "#  'max_depth': 83,\n",
    "#  'num_iterations': 200,\n",
    "#  'num_leaves': 82,\n",
    "#  'objective': 'tweedie'}\n",
    "\n",
    "# {'colsample_bytree': 0.41495667971584227,\n",
    "#  'learning_rate': 0.31545383409933103,\n",
    "#  'max_depth': 66,\n",
    "#  'min_child_weight': 16,\n",
    "#  'min_split_gain': 0.17155337882245103,\n",
    "#  'num_leaves': 48,\n",
    "#  'objective': 'regression',\n",
    "#  'subsample': 0.9107569191768009}\n",
    "\n",
    "lgb_hopt_2 = lgb.LGBMRegressor(**params)\n",
    "lgb_hopt_2.fit(X_train, y_train, eval_set=[(X_test, y_test), (X_train, y_train)], verbose=0)\n",
    "lgb.plot_metric(lgb_hopt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_lgb = lgb.Dataset(data = X_train, label = y_train, feature_name = list(X_train))\n",
    "# cv_results = lgb.cv(params,train_lgb,stratified=False, shuffle=False, num_boost_round=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(name=\"Vraies valeurs\", x=y_test.index, y=y_test, mode='lines+markers'))\n",
    "fig.add_trace(go.Scatter(name=\"LGBM\", x=y_test.index, y=model.predict(X_test).astype(int), mode='lines+markers'))\n",
    "fig.add_trace(go.Scatter(name=\"LGBM_HP\", x=y_test.index, y=lgb_hopt.predict(X_test).astype(int), mode='lines+markers'))\n",
    "#fig.add_trace(go.Scatter(name=\"LGBM_HP_2\", x=y_test.index, y=lgb_hopt_2.predict(X_test).astype(int), mode='lines+markers'))\n",
    "\n",
    "\n",
    "mean=(model.predict(X_test)+lgb_hopt_2.predict(X_test))/2\n",
    "#fig.add_trace(go.Scatter(name=\"LGBM_HP_mean\", x=y_test.index, y=mean.astype(int), mode='lines+markers'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Plot Aver. Perc. Err."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(name=\"LGBM\", x=X_test.index, y=predict_test(model, pd.DataFrame(), X_test, y_test)[\"absolute_error_%\"], mode='lines+markers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Learning & Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(name=\"Vraies valeurs\", x=pd.concat([X_train, X_test]).index, y=pd.concat([y_train, y_test]), mode='lines+markers'))\n",
    "fig.add_trace(go.Scatter(name=\"LGBM\", x=X_train.index, y=model.predict(X_train), mode='lines+markers', marker_color='springgreen'))\n",
    "fig.add_trace(go.Scatter(name=\"LGBM_hopt\", x=X_train.index, y=lgb_hopt.predict(X_train), mode='lines+markers', marker_color='indianred'))\n",
    "\n",
    "fig.add_trace(go.Scatter(name=\"LGBM_test\", x=X_test.index, y=model.predict(X_test).astype(int), mode='lines+markers', marker_color='#00CC96'))\n",
    "fig.add_trace(go.Scatter(name=\"LGBM_hopt_test\", x=X_test.index, y=lgb_hopt.predict(X_test).astype(int), mode='lines+markers',  marker_color='#EF553B'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tabl_err_all([model, lgb_hopt, lgb_hopt_2], X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tabl_err_all([model, lgb_hopt, lgb_hopt_2], X_train, y_train, X_test.loc['2023-01-01':'2023-04-01'], y_test.loc['2023-01-01':'2023-04-01']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save model\n",
    "joblib.dump(lgb_hopt, 'lgbm_reg_D_mail_invoicing.pkl')\n",
    "# load model\n",
    "gbm_pickle = joblib.load('lgbm_reg_D_mail_invoicing.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pred-reco--ft8wuni-py3.8",
   "language": "python",
   "name": "pred-reco--ft8wuni-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2eb74a4644ab1e2e677a407e6743403ad569464ea075d808389557d9c2b837ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
